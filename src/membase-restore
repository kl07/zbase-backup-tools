#!/usr/bin/python26
#Description: Incremental restore script

import time
import os
import consts
import sys
import signal
import socket
import tempfile
import Queue
from threading import Thread
from mc_bin_client import MemcachedClient
from logger import Logger
from config import Config
from util import natural_sortkey, zruntime_readkey, getcommandoutput, get_checkpoints_frombackup, gethostname

DAY_EPOCH = 60*60*24

def epoch2date(epoch):
    lt = time.gmtime(epoch)
    return time.strftime('%Y-%m-%d',lt)

class Restore:

    def __init__(self, hostname=None):
        self.processlist = Queue.Queue()
        self.s3_lock_file = None
        self.exit_status = 0
        self.restore_complete = False
        try:
            self.config = Config(consts.CONFIG_FILE)
            self.config.read()
            self.logger = Logger(tag = self.config.syslog_tag, level = self.config.log_level)
        except Exception, e:
            self.config.syslog_tag = consts.SYSLOG_TAG
            self.logger = Logger(tag = self.config.syslog_tag, level = self.config.log_level)
            self.logger.log("FAILED: Parsing config file (%s)" %(str(e)))
            self.exit(1)

        self.logger.log("=== Starting Membase Restore ===")

        self.cloud = self.config.cloud
        self.game_id = self.config.game_id
        if hostname == None:
            self.hostname = self.config.hostname
        else:
            self.hostname = hostname

        buffer_list = self.config.buffer_list.split(',')
        self.thread_count = len(buffer_list)
        self.free_buffer_list = Queue.Queue()
        self.download_queue = Queue.Queue()
        self.file_count = 0

        backup_process = self
        class stderrlog(object):
            def __init__(self, backup_process):
                self.parent = backup_process

            def write(self, err):
                self.parent.logger.log("FAILED: %s" %err)
                self.parent.exit(1)

        sys.stderr = stderrlog(backup_process)

        sys.stderr = stderrlog(self.logger) 
        for b in buffer_list:
            os.system("mkdir -p %s" %b)
            rm_buffer_cmd = "find %s -type f -delete" %b
            self.logger.log("Clearing buffer %s" %b)
            status, output = self.getstatusoutput(rm_buffer_cmd)
            if status > 0:
                self.logger.log("FAILED: Clearing buffer %s (%s)" %(b, output))
                self.exit(1)
            self.free_buffer_list.put(b)

        self.update_mappingserver()
        self.restore_queue = Queue.Queue()
        signal.signal(signal.SIGINT, self.graceful_exit)
        signal.signal(signal.SIGQUIT, self.graceful_exit)
        signal.signal(signal.SIGTERM, self.graceful_exit)

    def update_mappingserver(self):
        """
        Update mapping server env variable
        """
        for r in range(consts.ZRT_RETRIES):
            mapping_server = zruntime_readkey(self.config.zruntime_user,
                self.config.zruntime_passwd,
                self.config.zruntime_namespace,
                self.config.game_id,
                self.config.zruntime_mapperkey)
            if r > 0:
                self.logger.log("Retrying (%d) to ZRuntime read ..." %r)

            if mapping_server:
                break

            time.sleep(1)

        if not mapping_server:
            self.logger.log("FAILED: Unable to read from zRuntime")
            self.exit(1)
        else:
            os.environ["MAPPING_SERVER"] = mapping_server

    def setup_restore_env(self):
        self.logger.log("Setting up restore environment for membase server")
        if os.path.exists(consts.MEMCACHED_PID_FILE):
            memcached_pid = int(open(consts.MEMCACHED_PID_FILE).read())
            try:
                os.kill(memcached_pid, 0)
                self.logger.log("Found running membase instance. Terminating" \
                        " membase process")
                os.kill(memcached_pid, 9)
            except:
                pass

        self.logger.log("Cleaning up membase db directory")
        for dbpath in self.config.membase_db_paths:
            rm_db_cmd = "find %s -type f -delete" %dbpath
            status, output = self.getstatusoutput(rm_db_cmd)
            if status > 0:
                self.logger.log("FAILED: Failed to clean membase db directory" \
                        " (%s)" %output)
                self.exit(1)

        self.logger.log("Updating membase config file to start as restore mode")
        f = open(consts.MEMCACHED_SYSCONFIG_FILE)
        sysconfig = f.read().split(';')
        param, endlimiter = sysconfig[-1].split("'")
        sysconfig = sysconfig[:-1]
        sysconfig.append(param)
        restore_param = 'restore_mode=true'
        for p in sysconfig:
            if 'restore_mode' in p:
                sysconfig.remove(p)

        sysconfig.append(restore_param)
        sysconfig_str = ";".join(sysconfig)
        sysconfig_str += "'%s" %endlimiter
        f = open(consts.MEMCACHED_SYSCONFIG_FILE, 'w')
        f.write(sysconfig_str)
        f.close()
        self.logger.log("Starting membase")
        start_membase_cmd = "/sbin/service memcached start"
        status = os.system(start_membase_cmd)
        if status > 0:
            self.logger.log("Unable to start membase")
            self.exit(1)

        for i in range(120):
            try:
                mb = MemcachedClient(host='127.0.0.1', port=11211)
                mb.stats('')
                break
            except:
                if i == 119:
                    start_membase_cmd = "/sbin/service memcached restart"
                    status = os.system(start_membase_cmd)
                    if status > 0:
                        self.logger.log("Unable to start membase")
                        self.exit(1)
                time.sleep(1)

        register_tap_cmd = "python26 %s -h 0:11211 -r %s -c" \
                %(consts.PATH_MBTAP_REGISTER_EXEC, consts.BACKUP_TAPNAME)
        self.logger.log("Registering Backup TAP Cursor")
        status, output = self.getstatusoutput(register_tap_cmd)
        if status > 0:
            self.logger.log("FAILED: Backup tap registration (%s)" %output)
            self.exit(1)

    def _list_s3_files(self, s3path, complete=True, only_index=False):
        exclude_list = []
        found_dot_done = False
        ls_cmd = "%s ls %s" %(consts.PATH_S3CMD_EXEC, s3path)
        if not only_index:
            self.logger.log("Executing command %s" %ls_cmd)
        status, output = self.getstatusoutput(ls_cmd)
	if status !=0:
            self.logger.log("FAILED: Listing files failed for %s (%s)" %(s3path, output))
            return False
	else:
            lines = output.split('\n')
            files = map(lambda x: 's3://'+x.split('s3://')[-1], lines)

            for f in files:
                if f.endswith('done'):
                    found_dot_done = True

                if f.endswith(consts.DEL_MANIFEST):
                    self.logger.log("Downloading delete manifest file")
                    dlmanifest_cmd = '%s get %s /tmp/manifest.del' \
                            %(consts.PATH_S3CMD_EXEC, f)
                    status, output = self.getstatusoutput(dlmanifest_cmd)
                    if status > 0:
                        self.logger.log("WARNING: Unable to download" \
                                "delete manifest file (%s)" %output)
                    else:
                        exclude_lines = open('/tmp/manifest.del').readlines()
                        for l in exclude_lines:
                            exclude_list.append("%s%s" %(s3path, os.path.basename(l.strip())))

            if complete == False:
                complete = found_dot_done

            if only_index:
                mbb_files = filter(lambda x: x.endswith('.split'), files)
                #Remove the -00001.mbb from exclude list files and create uniq .split filenames 
                exclude_list = set(map(lambda x: "%s.split" %"-".join(x.split('-')[:-1]), exclude_list))
                mbb_files = list(set(mbb_files) - set(exclude_list))
            else:
                mbb_files = filter(lambda x: x.endswith('.mbb'), files)
                mbb_files = list(set(mbb_files) - set(exclude_list))

            if complete == False:
                return []

            return mbb_files

    def _download_file(self, s3path, filepath):
        get_cmd = "%s sync %s %s" %(consts.PATH_S3CMD_EXEC, s3path, filepath)
        retries = self.config.download_retries
        self.logger.log("Executing command %s" %get_cmd)
        for i in range(retries):
            if i > 0:
                self.logger.log("Retrying download for %s" %s3path)
                self.update_mappingserver()

            status, output = self.getstatusoutput(get_cmd)
            self.logger.log("Downloading file %s to %s" %(s3path, filepath))
            if status == 0:
                break

	if status > 0:
            self.logger.log("FAILED: Downloading file %s failed (%s)" %(s3path, output))
            return False
        else:
            self.logger.log("SUCCESS: Completed downloading file %s to %s (retries=%d)" %(s3path, filepath, i))
            return True

    def graceful_exit(self, signum=None, frame=None):
        for process in self.processlist.queue:
            try:
                process.terminate()
            except:
                pass
        self.clean_s3_lock()
        if os.path.exists(consts.MBRESTORE_PID_FILE):
            os.unlink(consts.MBRESTORE_PID_FILE)
        self.logger.log("Restore process terminated")
        os._exit(self.exit_status)

    def exit(self, status):
        self.exit_status = status
        self.graceful_exit()

    def getstatusoutput(self, cmd):
        return getcommandoutput(cmd, self.processlist)

    def clean_s3_lock(self):
        if self.s3_lock_file:
            rm_cmd = "%s del %s" %(consts.PATH_S3CMD_EXEC, self.s3_lock_file)
            for i in range(self.config.upload_retries):
                status, output = self.getstatusoutput(rm_cmd)
                if i > 0:
                    self.logger.log("Retrying to remove incremental backup directory lock in s3")
                    self.update_mappingserver()
                if status == 0:
                    return 
            self.logger.log("FAILED: Unable to remove s3 incremental directory lock")
            os._exit(1)

    def verify_split_index(self, base_s3_path, files):
        backup_index_list = self._list_s3_files(base_s3_path, True, True)
        tmp = files[:]
        for index_file in backup_index_list:
            tmpfile = tempfile.mktemp()
            if not self._download_file(index_file, tmpfile):
                self.logger.log("FAILED: Unable to download split_index file, %s" %index_file)
                self.exit(1)
            files_in_index = map(lambda x: "%s%s" %(base_s3_path, x.strip()), open(tmpfile).readlines())
            os.unlink(tmpfile)

            for f in files_in_index:
                if f in tmp:
                    tmp.remove(f)
                else:
                    self.logger.log("FAILED: Split verification - %s not found" %f)
                    self.exit(1)

        if len(tmp) != 0:
            self.logger.log("FAILED: Split verification - found invalid files %s" %str(tmp))
            self.exit(1)

    def fetch_backuplist(self):
        """
        Get the list of backup files which are to be downloaded
        """
        self.logger.log("Fetching Backup list from S3")
        epoch = time.time()
        self.logger.log("Fetching list of incremental backups")
        incremental_backup_s3path = 's3://%s/%s/%s/%s/' %(self.game_id, self.hostname, self.cloud, consts.INCR_DIRNAME)
        incremental_backup_list = self._list_s3_files(incremental_backup_s3path)
        if not incremental_backup_list:
            self.logger.log("Could not find any incremental backups")
            incremental_backup_list = []

        self.verify_split_index(incremental_backup_s3path, incremental_backup_list)
        hostname = gethostname()
        f = open('/tmp/lock-%s' %hostname,'w')
        f.close()
        self.s3_lock_file = "%slock-%s" %(incremental_backup_s3path, hostname)
        lock_cmd = "%s put /tmp/lock-%s %s" %(consts.PATH_S3CMD_EXEC, hostname, incremental_backup_s3path)
        status, output = self.getstatusoutput(lock_cmd)
        if status > 0:
            self.logger.log("FAILED: Unable to put incremental backup directory lock in s3")
            self.s3_lock_file = None
            self.exit(1)
        else:
            self.logger.log("Locked incremental backup directory in s3")

        master_backup_epoch = epoch
        self.logger.log("Searching for master backup")
        for attempt in xrange(consts.MAX_BACKUP_SEARCH_TRIES):
            date = epoch2date(master_backup_epoch)
            master_s3_path = 's3://%s/%s/%s/%s/%s/' %(self.game_id, self.hostname, self.cloud, consts.MASTER_DIRNAME, date)
            master_backup_list = self._list_s3_files(master_s3_path, False)
            if len(master_backup_list):
                self.logger.log("Found master backup for date, %s" %epoch2date(master_backup_epoch))
                self.verify_split_index(master_s3_path, master_backup_list)
                master_backup_epoch += DAY_EPOCH
                break
            else:
                master_backup_epoch -= DAY_EPOCH
                if attempt == consts.MAX_BACKUP_SEARCH_TRIES - 1:
                    self.logger.log("Could not find master backup. Maximum tries exceeded")
                    self.exit(1)
                    master_backup_list = []

        self.logger.log("Fetching list of periodic backups")
        periodic_backup_list = []
        for date_epoch in xrange(int(master_backup_epoch), int(epoch) + DAY_EPOCH, DAY_EPOCH):
            date = epoch2date(date_epoch)
            periodic_s3_path = 's3://%s/%s/%s/%s/%s/' %(self.game_id, self.hostname, self.cloud, consts.PERIODIC_DIRNAME, date)
            periodic_file_list = self._list_s3_files(periodic_s3_path, False)
            self.verify_split_index(periodic_s3_path, periodic_file_list)
            if not periodic_file_list:
                self.logger.log("Could not find periodic backup for %s" %date)
            periodic_backup_list.extend(periodic_file_list)

        incremental_backup_list.sort(key=natural_sortkey)
        incremental_backup_list.reverse()
        master_backup_list.sort(key=natural_sortkey)
        master_backup_list.reverse()
        periodic_backup_list.sort(key=natural_sortkey)
        periodic_backup_list.reverse()
        return incremental_backup_list + periodic_backup_list + master_backup_list

    def populate_queue(self):
        backup_files = self.fetch_backuplist()
        if len(backup_files) == 0:
            self.logger.log("No backup files found.")
            self.exit(2)

        for f in backup_files:
            self.logger.log("Found file %s" %f)

        if not backup_files:
            self.exit(1)

        for i,f in enumerate(backup_files):
            self.download_queue.put((i, f, 'backup-%05d.mbb' %i))

        self.file_count = len(self.download_queue.queue)

    def _do_online_restore(self, backup_file, complete=False):
        self.logger.log("Performing restore from %s" %backup_file)
        if complete:
            restore_cmd = "python26 %s -h 127.0.0.1:11211 -c %s" %(consts.PATH_MBRESTORE_EXEC, backup_file)
        else:
            restore_cmd = "python26 %s -h 127.0.0.1:11211 %s" %(consts.PATH_MBRESTORE_EXEC, backup_file)

        self.logger.log("Executing command %s" %restore_cmd)
        status, output = self.getstatusoutput(restore_cmd)
        if status == 0:
            return True
        else:
            self.logger.log("FAILED: Executing command %s (%s)"  %(restore_cmd, output))
            return False

    def perform_restore(self):
        try:
            shard = 0
            last_checkpoint = None
            last_file_checkpoints = None
            while shard < self.file_count:
                self.logger.log("RESTORE: Waiting for backup file %d/%d" %(shard+1, self.file_count))
                restore_list = list(self.restore_queue.queue)
                restore_list.sort()
                if len(restore_list) > 0 and shard == restore_list[0][0]:
                    self.logger.log("Processing backup file %d/%d" %(shard+1, self.file_count))
                    backup = restore_list[0]
                    self.restore_queue.queue.remove(backup)
                    backup_file = backup[1]
                    buffer_path = backup[2]
                    try:
                        checkpoints = get_checkpoints_frombackup(backup_file)
                    except Exception, e:
                        self.logger.log("FAILED: sqlite file %s is corrupt (%s)" %(backup_file, str(e)))
                        self.exit(1)

                    if shard > 0:
                        #Ignore ordering verification we last file has got same checkpoints (splits of same backup)
                        if not last_file_checkpoints == checkpoints:
                            if not (last_checkpoint == checkpoints[-1] or last_checkpoint - 1 == checkpoints[-1]):
                                self.logger.log("FAILED: Checkpoint order mismatch. Last file checkpoint: %d Current file checkpoint: %d" %(last_checkpoint, checkpoints[-1]))
                                self.exit(1)
                    else:
                        try:
                            f = open(consts.LAST_CHECKPOINT_FILE, 'w')
                            f.write(str(checkpoints[-1]))
                            f.close()
                        except Exception, e:
                            self.logger.log("FAILED: Unable to write last_checkpoint file (%s)" %str(e))
                            self.exit(1)

                    last_checkpoint = checkpoints[0]
                    last_file_checkpoints = checkpoints
                    self.logger.log("Checkpoints in the current backup-file %s : %s" %(backup_file, str(checkpoints)))

                    complete = False
                    #NOTE: Do not switch back to normal mode from restore mode
                    #Master server will take care of restore mode switch
                    #if shard == self.file_count - 1:
                    #    complete = True
                    #else:
                    #    complete = False

                    status = self._do_online_restore(backup_file, complete)
                    if status:
                        os.unlink(backup_file)
                        self.free_buffer_list.put(buffer_path)
                        shard +=1
                    else:
                        self.exit(1)

                time.sleep(1)

            self.restore_complete = True
        except Exception, e:
            self.logger.log("Thread stopped with exception (%s)" %str(e))
            self.exit(1)

    def download_files(self):
        while True:
            self.logger.log("Waiting for obtaining download buffer")
            backup = self.download_queue.get()
            self.logger.log("SUCCESS: Attempt to obtain a file for download %s" %str(backup))
            buffer_path = self.free_buffer_list.get()
            self.logger.log("Obtained buffer %s" %buffer_path)
            status = self._download_file(backup[1], '%s/%s' %(buffer_path, backup[2]))
            if status:
                self.restore_queue.put((backup[0], '%s/%s' %(buffer_path, backup[2]), buffer_path))
            else:
                self.exit(1)

            self.download_queue.task_done()

    def main(self):
        start_time = int(time.time())
        if os.getuid() != 0:
            print "Please run as root"
            sys.exit(1)

        if os.path.exists(consts.MBRESTORE_PID_FILE):
            pid = int(open(consts.MBRESTORE_PID_FILE, 'r').read())
            try:
                os.kill(pid, 0)
                self.logger.log("Restore process is already running with PID %d" %pid)
                sys.exit(1)
            except:
                pass
        fd = open(consts.MBRESTORE_PID_FILE,'w')
        fd.write(str(os.getpid()))
        fd.close()

        self.populate_queue()
        for i in range(self.thread_count):
            t = Thread(target=self.download_files)
            t.daemon = True
            t.start()
        t = Thread(target=self.perform_restore)
        t.daemon = True
        t.start()

        while not self.restore_complete:
            time.sleep(10)

        end_time = int(time.time())
        self.clean_s3_lock()

        f = open(consts.MEMCACHED_SYSCONFIG_FILE)
        sysconfig = f.read().split(';')
        param, endlimiter = sysconfig[-1].split("'")
        sysconfig = sysconfig[:-1]
        sysconfig.append(param)
        for p in sysconfig:
            if 'restore_mode' in p:
                sysconfig.remove(p)

        sysconfig_str = ";".join(sysconfig)
        sysconfig_str += "'%s" %endlimiter
        f = open(consts.MEMCACHED_SYSCONFIG_FILE, 'w')
        f.write(sysconfig_str)
        f.close()
        self.logger.log("Restore completed successfully in %d seconds" %(end_time-start_time))
        self.exit(0)

if __name__ == '__main__':
    if os.getuid() != 0:
        print "Please run as root"
        sys.exit(1)


    if os.path.exists(consts.MBRESTORE_PID_FILE):
        pid = int(open(consts.MBRESTORE_PID_FILE).read())
        try:
            os.kill(pid, 0)
            print "Restore process with PID:%d is already running" %pid
            os._exit(1)
        except:
            pass

    args = sys.argv[1:]
    hostname = None
    if len(args) > 0:
        if args[0] == '-h' and len(args) == 2:
            hostname = args[1]
        else:
	    print "Usage: %s -h slave-hostname" %(sys.argv[0])
	    sys.exit(0)

    restore = Restore(hostname)
    restore.setup_restore_env()
    restore.main()

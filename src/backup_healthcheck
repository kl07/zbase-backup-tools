#!/usr/bin/env python
#Description: Verify backups and generate a report 

import consts
from util import get_checkpoints_frombackup
from util import getcommandoutput
import os
import datetime
import glob
import Queue
import logging
import time
import sys 
import getopt
import signal

global pids
pids = Queue.Queue()

def clear(*x):
    if len(pids.queue):
        p = pids.get()
        p.terminate()
    print "Received exit signal"


for s in [signal.SIGINT, signal.SIGTERM, signal.SIGQUIT]:
    signal.signal(s, clear)


class Verifier:
    """
    Verifier class  - provides methods for listing backup, verify checkpoints,
    count unique keys
    """
    def __init__(self, game, cloud, report_dir, calc_keys=False, date=None):
        self.backup_root = os.path.join(consts.STORAGE_SERVER_ROOT, game)
        self.hosts = os.listdir(self.backup_root)
        self.game = game 
        self.cloud = cloud
        self.report_dir = report_dir
        self.calc_keys = calc_keys
        self.logger = logging.getLogger('BackupVerifier')
        formatter = logging.Formatter('%(message)s')
        self.logger.setLevel(logging.DEBUG)
        self.date = date
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')
        handler_file = logging.FileHandler(os.path.join(self.report_dir,
                                    "%s-%s.%s.rep" %(cloud, game, timestamp)))
        handler_file.setFormatter(formatter)
        self.logger.addHandler(handler_file)

    def log(self, msg):
        """
        Logger methods
        """
        self.logger.info(msg)
        print msg

    def set_hosts(self, hosts):
        """
        Set host list for verification
        """
        self.hosts = hosts

    def inspector(self):
        """
        Scan backup and generate health
        """
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.log("=========== BACKUP HEALTH CHECK on %s ===========" %timestamp)
        for host in self.hosts:
            st_time = time.time()
            self.log("##### Starting health scan for HOSTNAME: %s #####" %host)
            status, backups = self.backup_walk(host, self.date)
            self.checkpoint_walk(backups, host)
            if self.calc_keys:
                self.log("INFO:%s Calculating unique keys count" %host)
                count = self.count_keys(backups, host)
                if count != -1:
                    self.log("INFO:%s Total keys count:%d" %(host, count))
            end_time = time.time()
            self.log("##### Completed health scan for HOSTNAME: %s in %d" \
            " seconds #####" %(host, end_time-st_time))
            self.log(" ")

    def verify_split_index(self, split_index_file, files, hostname):
        """
        Verify backups with split index file
        NOTE: Code borrowed from membase-restore
        """

        status = True
        dir = os.path.dirname(split_index_file)
        files_in_index = map(lambda x: os.path.join(dir, x.strip()), open(split_index_file).readlines())
        tmp = files[:]
        for f in files_in_index:
            if f in tmp:
                tmp.remove(f)
            else:
                if status:
                    self.log("CRITICAL:%s split index verification failed" %hostname)
                status  = False

        if len(tmp) != 0:
            self.log("CRITICAL:%s split index verification failed - found" \
                    " invalid files %s" %(hostname, str(tmp)))
            status = False

        return status

    def checkpoint_walk(self, backups, hostname):
        """
        Walk through the checkpoints from backups and check continuity
        NOTE: Code borrowed from membase-restore
        """
        last_file_checkpoints = None
        last_file = None
        for backup in backups:
            checkpoints = get_checkpoints_frombackup(backup)
            if last_file_checkpoints:
                if not (checkpoints == last_file_checkpoints):
                    if not (last_file_checkpoints[0] == checkpoints[-1] \
                            or (last_file_checkpoints[0] - 1) == checkpoints[-1]):
                        self.log("CRITICAL:%s Checkpoint order mismatch between" \
                        "%s (%s) and %s (%s)" %(hostname, last_file, \
                            last_file_checkpoints, backup, checkpoints))
            last_file = backup
            last_file_checkpoints = checkpoints

    def count_keys(self, files, hostname):
        """
        Count unique keys from given backup list
        """
        try:
            status, output = getcommandoutput('%s %s' %(consts.PATH_COUNTKEYS_EXEC, " ".join(files)), pids)
        except:
            if len(pids.queue):
                p = pids.get()
                p.terminate()
            return -1

        if status > 0:
            self.log("CRITICAL:%s Unable to count keys (%s)" %(hostname, output))
            return -1
        else:
            if "Error" in output:
                self.log("CRITICAL:%s Unable to count keys (%s)" %(hostname, output))
                return -1
            else:
                return int(output)
    
    def backup_walk(self, hostname, date=None):
        """
        Walk through the backups - incremental, daily and nearest master
        and build the backup list 
        NOTE: Code borrowed from master-merge
        """
        backup_list = []
        master_list = []
        periodic_list = []
        exclude_list = []
        if not date:
            datetime_object = datetime.date.today()
        else:
            year, month, day = map(lambda x: int(x), self.date.split('-'))
            datetime_object = datetime.date(year, month, day)

        path = os.path.join(self.backup_root, hostname, self.cloud, consts.INCR_DIRNAME)
        manifest_file = os.path.join(path, 'manifest.del')
        if os.path.exists(manifest_file):
            exclude_list = map(lambda x: x.strip(), open(manifest_file).readlines())

        backup_list = list(set(glob.glob("%s/*.mbb" %path)) - set(exclude_list))
        backup_list.sort()
        backup_list.reverse()
        self.log("INFO:%s Found %d incremental backups" \
                            %(hostname, len(backup_list))) 
        status = True
        found_master = False
        for i in range(consts.MAX_BACKUP_LOOKUP_DAYS+1):
            difference = datetime.timedelta(days=-(i))
            datestamp = (datetime_object + difference).strftime('%Y-%m-%d')
            path = os.path.join(self.backup_root, hostname, self.cloud, consts.MASTER_DIRNAME, datestamp)
            if os.path.exists(os.path.join(path, "done")):
                master_list = glob.glob("%s/*.mbb" %path)
                if len(master_list) > 0:
                    self.log("INFO:%s Found master backup for date:%s" \
                            %(hostname, datestamp)) 
                    split_index = glob.glob("%s/*.split" %path)
                    if len(split_index) == 0:
                        self.log("CRITICAL:%s split index file not found (%s)" \
                                %(hostname, path))
                        status = False
                    else:
                        status = self.verify_split_index(split_index[0],
                                master_list, hostname)
                    found_master = True

            if found_master:
                master_list.sort()
                master_list.reverse()
                backup_list.extend(master_list)
                break

            path = os.path.join(self.backup_root, hostname, self.cloud, consts.PERIODIC_DIRNAME, datestamp)
            if os.path.exists(os.path.join(path, "done")):
                periodic_list = glob.glob("%s/*.mbb" %path)
                self.log("INFO:%s Found daily backup for date:%s" \
                            %(hostname, datestamp)) 
                split_index = glob.glob("%s/*.split" %path)
                if len(split_index) == 0:
                    self.log("CRITICAL:%s split index file not found (%s)" \
                            %(hostname, path))
                    status = False
                else:
                    status = self.verify_split_index(split_index[0], periodic_list, hostname)

                periodic_list.sort()
                periodic_list.reverse()
                backup_list.extend(periodic_list)
            else:
                self.log("WARNING:%s Daily backup for %s not found" %(hostname, datestamp))

        if not found_master:
            self.log("CRITICAL:%s Master backup not found" \
                            %(hostname))

        return status, backup_list

def usage(cmd, msg=None):
    print "Usage: %s -g game_id -c cloud_id -o report_dir [-k] [-d date] [-h host1,host2,..]" %cmd
    if msg:
        print "ERROR: %s" %msg
    print
    sys.exit(1)

if __name__ == '__main__':
    options = {}
    hosts = []
    count = False
    date = None
    try:
        opts, args = getopt.getopt(sys.argv[1:], 'g:c:o:h:kd:')
    except getopt.GetoptError, e:
        usage(sys.argv[0], e.msg)
    try:
        for (o,a) in opts:
            if o == '-g':
                options['game'] = a
            elif o == '-c':
                options['cloud'] = a
            elif o == '-o':
                options['outdir'] = a
                if not os.path.exists(a):
                    print "Unable to find report directory"
                    usage(sys.argv[0])

            elif o == '-h':
                hosts = a.split(',')

            elif o == '-k':
                count = True

            elif o == '-d':
                date = a
                try:
                    if len(a.split('-')) != 3:
                        raise Exception
                except:
                    usage(sys.argv[0], "Invalid date")

    except Exception, e:
        print "ERROR: %s" %str(e)
        sys.exit(1)

    if len(options) != 3:
        usage(sys.argv[0])

    v = Verifier(options['game'], options['cloud'], options['outdir'], count, date)
    if len(hosts):
        v.set_hosts(hosts)
    v.inspector()

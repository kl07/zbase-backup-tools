#!/usr/bin/python
#Description: Backup s3 uploader Daemon

import sys
import os
import re
import time
import signal
import socket
import Queue
import glob
import datetime
import random
from threading import Thread
from pwd import getpwnam
import consts
from config import Config
from logger import Logger
from mc_bin_client import MemcachedClient
from util import setup_sqlite_lib, getcommandoutput, get_checkpoints_frombackup
from backuplib import BackupFactory

# Setup the sqlite3 LD_LIBRARY_PATH before importing sqlite3
setup_sqlite_lib()
import sqlite3

class BackupProcess:
    """
    Create backups and upload incremental backups to S3
    """
    def __init__(self):
        self.exit_status = 0
        self.upload_only_mode = False
        self.first_master_backup = False
        self.resume_process_mode = False
        self.processlist = Queue.Queue()
        try:
            self.config = Config(consts.CONFIG_FILE)
            self.config.read()
            self.logger = Logger(tag = self.config.syslog_tag, level = self.config.log_level)
        except Exception, e:
            self.config.syslog_tag = consts.SYSLOG_TAG
            self.logger = Logger(tag = self.config.syslog_tag, level = self.config.log_level)
            self.logger.log("FAILED: Parsing config file (%s)" %(str(e)))
            self.exit(1)

        class stderrlog(object):
            def __init__(self, logger):
                self.logger = logger
            def write(self, err):
                self.logger.log("FAILED: %s" %err)

        sys.stderr = stderrlog(self.logger)
        try:
            self.hostname = socket.gethostname()
            raw_buffer_list = self.config.buffer_list.split(',')
            buffer_list = set(raw_buffer_list)
            if len(buffer_list) != len(raw_buffer_list):
                self.logger.log("FAILED: Duplicate buffer list found in the config")
                self.exit(1)

            f = open('/etc/mtab')
            lines = f.readlines()
            tmpfs_locations = map(lambda y: y.split(' ')[1], filter(lambda x: ' tmpfs ' in x, lines))
            for b in buffer_list:
                if len(filter(lambda x: x in b, tmpfs_locations)) == 0:
                    self.logger.log("FAILED: Buffer %s is not a tmpfs location" %b)
                    self.exit(1)

            if not os.path.exists(consts.PATH_S3CMD_CONFIG):
                self.logger.log("FAILED: s3cmd_zynga configuration not found at %s" %(consts.PATH_S3CMD_CONFIG))
                self.exit(1)

            self.free_buffer_list = Queue.Queue()
            self.backup_queue = Queue.Queue()
            for b in buffer_list:
                self.free_buffer_list.put(b)

        except Exception, e:
            self.logger.log("FAILED: %s " %str(e))
            self.exit(1)

        self.logger.log("=== Starting Membase Backup Daemon ===")
	signal.signal(signal.SIGINT, self.graceful_exit)
	signal.signal(signal.SIGQUIT, self.graceful_exit)
	signal.signal(signal.SIGTERM, self.graceful_exit)

    def getstatusoutput(self, cmd):
        return getcommandoutput(cmd, self.processlist)

    def resume_process(self):
        self.logger.log("Looking for backup files to resume upload process")
        dirpath_suffix = '%s/%s/%s/%s' %(self.config.game_id, self.hostname, self.config.cloud, consts.INCR_DIRNAME)

        for b in list(self.free_buffer_list.queue):
            try:
                flist = list(set(map(lambda x: x[:-10]+'-%.mbb',
                    glob.glob("%s/*.mbb"
                    %os.path.join(b, dirpath_suffix)))))
            except:
                return

            for f in flist:
                self.resume_process_mode = True
                filepath = os.path.join(b,dirpath_suffix,f)
                valid = True
                files = glob.glob(f.replace('%','*'))
                for bf in files:
                    if not self._is_backup_valid(bf):
                        valid = False

                if valid:
                    self.logger.log("Found backup file %s. Pusing to uploader queue" %flist[0])
                    item = (filepath, b)
                    self.backup_queue.put(item)
                else:
                    for bf in files:
                        self._remove_file(bf)

                if b in self.free_buffer_list.queue:
                    self.free_buffer_list.queue.remove(b)

    def _upload_file(self, filepath, buffer_path):
        retries = self.config.upload_retries
        root_path = buffer_path
        if root_path[-1] == '/':
            root_path = root_path[:-1]

        regex = re.compile('^%s/(.*.mbb)' %root_path)
        s3suffix_path = regex.findall(filepath)[0]
        s3path = "s3://%s" %(s3suffix_path)
        self.logger.log("ATTEMPT: Uploading %s to %s " %(filepath, s3path))
        upload_cmd = "%s sync %s %s" %(consts.PATH_S3CMD_EXEC, filepath, s3path)
        self.logger.log("Executing command, %s " %upload_cmd)
        for i in range(retries):
            if i > 0:
                self.logger.log("Retrying upload for %s " %filepath)

            self.logger.log("Uploading file %s to %s " %(filepath, s3path))
            status, output = self.getstatusoutput(upload_cmd)
            if status == 0:
                break

        if status > 0:
            self.logger.log("FAILED: Upload to S3 failed for backup file %s (%s)" %(filepath, output))
            return False
        else:
            self.logger.log(output)
            self.logger.log("SUCCESS: Uploading %s to %s (retries=%d)" %(filepath, s3path, i))
        return True


    def _is_backup_valid(self, filepath):
        try:
            db = sqlite3.connect(filepath)
            cursor = db.execute("select count(*) from cpoint_op;")
            mutation_count = cursor.fetchone()[0]
            if mutation_count == 0:
                return False
            else:
                return True

        except Exception, e:
            self.logger.log("FAILED: sqlite validation failed for %s (%s)" %(filepath, str(e)))
            return False

    def _remove_file(self, filepath):
        self.logger.log("Removing backup file, %s " %filepath)
        try:
            os.unlink(filepath)
            return True
        except Exception, e:
            self.logger.log("FAILED: Unable to remove file %s (%s)" %(filepath, str(e)))
            return False

    def _ss_remove_file(self, buffer_path, filepath):
        retries = self.config.upload_retries

        root_path = buffer_path
        if root_path[-1] == '/':
            root_path = root_path[:-1]

        regex = re.compile('^%s/(.*/[^/]*.mbb)' %root_path)
        s3suffix_path = regex.findall(filepath)[0]
        s3path = "s3://%s" %(s3suffix_path)
        del_cmd = "%s del %s" %(consts.PATH_S3CMD_EXEC, s3path)
        self.logger.log("Executing command, %s " %del_cmd)
        for i in range(retries):
            if i > 0:
                self.logger.log("Retrying delete for %s " %f)
            status, output = self.getstatusoutput(del_cmd)
            if status == 0:
                break
        return status

    def upload_process_thread(self):
        upload_interval = self.config.upload_interval_mins*60
        while True:
            start_time = time.time()
            if self.upload_only_mode and len(self.backup_queue.queue) == 0:
                os._exit(0)

            self.logger.log("Waiting for obtaining backup file")
            backup_item = self.backup_queue.get()
            (backup_filepath, buffer_path) = backup_item
            status = self._upload_file(backup_filepath, buffer_path)
            if status:
                self.free_buffer_list.put(buffer_path)
                for f in glob.glob(backup_filepath.replace('%','*')):
                    self._remove_file(f)
            else:
                self.exit(1)

            if self.resume_process_mode:
                if len(self.backup_queue.queue) == 0:
                    self.resume_process_mode = True
                else:
                    continue

            if self.upload_only_mode:
                if len(self.backup_queue.queue) == 0:
                    os._exit(0)
                continue

            end_time = time.time()
            total_time = int(end_time-start_time)
            sleep_time = upload_interval - total_time
            if sleep_time > 0:
                time.sleep(sleep_time)

    def graceful_exit(self, signum=None, frame=None):
        for process in self.processlist.queue:
            try:
                process.terminate()
            except:
                pass

        self.logger.log("Backup process terminated")
        if os.path.exists(consts.MBBACKUP_PID_FILE):
            os.unlink(consts.MBBACKUP_PID_FILE)
        os._exit(self.exit_status)

    def exit(self, status):
        self.exit_status = status
        self.graceful_exit()

    def mbstats(self, item):
        try:
            mb = MemcachedClient(host='127.0.0.1', port=11211)
            return mb.stats(item)
        except Exception, e:
            self.logger.log("FAILED: Unable to query stats from Membase:11211 (%s)" %str(e))
            self.exit(1)

    def is_cursor_valid(self, cursor_name):
        try:
            checkpoint_stats = self.mbstats('checkpoint')
        except Exception:
            self.logger.log("FAILED: Could not fetch tap stats from membase server")
            self.exit(1)

        if len(filter(lambda x: 'eq_tapq:%s' %(cursor_name) in x, checkpoint_stats)):
            return True
        return False

    def is_closed_checkpoint_moved(self):
        last_checkpoint = 0
        if os.path.exists(consts.LAST_CHECKPOINT_FILE):
            last_checkpoint = int(open(consts.LAST_CHECKPOINT_FILE).read())

        stats = self.mbstats('checkpoint')
        current_checkpoint = int(stats['vb_0:last_closed_checkpoint_id'])
        self.logger.log("Last closed checkpoint ID: %d, Current closed checkpoint ID: %d" %(last_checkpoint, current_checkpoint))
	self.current_checkpoint = current_checkpoint
        if last_checkpoint != current_checkpoint:
            return True
        else:
            return False

    def is_master_backup(self):
        now = datetime.datetime.now()
        if os.path.exists(consts.LAST_MASTER_BACKUP_TIME):
            timestamp = open(consts.LAST_MASTER_BACKUP_TIME, 'r').read() 
            year, month, day, hr, mins, secs, msecs = map(lambda x: int(x), timestamp.split('-'))
            last_backup_time = datetime.datetime(year, month, day, hr, mins, secs, msecs)
            if (now - last_backup_time).total_seconds() >= int(self.config.master_backup_interval_days*24*60*60):
                return True

        return False

    def _create_master_backup_marker(self, timestamp=None):
        if timestamp == None:
            rand = random.random(0, self.config.master_backup_interval_days)
            last_backup_time = datetime.datetime.now() + datetime.timedelta(days=-(rand))
            timestamp = last_backup_time.strftime('%Y-%m-%d-%H-%M-%S-000000')

        f = open(consts.LAST_MASTER_BACKUP_TIME, 'w')
        f.write(timestamp)
        f.close()


    def backup_process_thread(self):
        interval = self.config.backup_interval_mins*60
        while True:
            start_time = time.time()
            if os.path.exists(consts.MBRESTORE_PID_FILE):
                pid = int(open(consts.MBRESTORE_PID_FILE).read())
                try:
                    os.kill(pid, 0)
                    self.logger.log("Restore process is on progress. Exiting")
                    self.exit(1)
                except:
                    pass

            if self.is_cursor_valid(consts.REPLICATION_TAPNAME):
                self.logger.log("The host is a master. Exiting")
                self.exit(1)

            if not self.is_cursor_valid(consts.BACKUP_TAPNAME):
                self.logger.log("FAILED: Registered TAP for Backup not found")
                self.exit(1)

            master_backup_attempt = self.is_master_backup()
            if self.is_closed_checkpoint_moved() or master_backup_attempt or self.first_master_backup:

                now = time.localtime(time.time())
                datetimestamp = time.strftime('%Y-%m-%d %H:%M:%S', now)
                backup_filename = time.strftime('backup-%Y-%m-%d_%H:%M:%S-%.mbb',now)
                if self.first_master_backup or master_backup_attempt:
                    backup_dir = os.path.join(self.config.game_id,
                            self.hostname, self.config.cloud,
                            consts.MASTER_DIRNAME, "%s"
                            %(time.strftime('%Y-%m-%d', now)))
                    self.first_master_backup = False
                    full_backup = True
                else:
                    backup_dir = os.path.join(self.config.game_id, self.hostname, self.config.cloud, consts.INCR_DIRNAME)
                    full_backup = False

                backup_filepath = os.path.join(backup_dir, backup_filename)
                status = self._take_backup(backup_filepath, datetimestamp, full_backup)
                if status:
                    if self.first_master_backup:
                        self._create_master_backup_marker()
                        self.first_master_backup = False
                    elif master_backup_attempt:
                        self._create_master_backup_marker(time.strftime('%Y-%m-%d-%H-%M-%S-000000',now))
                else:
                    self.exit(1)
            else:
                self.logger.log("Closed checkpoint hasn't been moved. Skipping backup")

            end_time = time.time()
            total_time = int(end_time - start_time)
            sleep_time = interval - total_time
            if sleep_time > 0:
                time.sleep(sleep_time)
            else:
                time.sleep(interval+sleep_time)

    def _take_backup(self, backup_filepath, datetimestamp, full_backup):
        """
        Create incremental backup
        """
        total_size = 0
        size = 0
        checkpoints = []

        if full_backup:
            btype = "full"
        else:
            btype =  "incremental"

        start_time = time.time()
        self.logger.log("==== START BACKUP ====")
        self.logger.log("Creating %s Backup for %s " %(btype, datetimestamp))

        try:
            bf_instance = BackupFactory(backup_filepath, btype,
                    consts.BACKUP_TAPNAME, self.logger, 'localhost',11211) 
        except Exception, e:
            self.logger.log("FAILED: Initializing backup factory instance")
            self.log(str(e))
            self.exit(1)

        while not bf_instance.is_complete():
	    self.logger.log("Waiting for obtaining free ramfs buffer")
	    buffer_path = self.free_buffer_list.get()
            self.logger.log("Obtained buffer %s " %buffer_path)
            try:
                filepath = bf_instance.create_next_split(buffer_path)
                size = os.stat(filepath).st_size
                total_size += size

                if size <= 4096:
                    if not self._is_backup_valid(filepath):
                        self.logger.log("FAILED: Backup size is %d" %size)
                        raise Exception("Backup is invalid")

                try:
                    checkpoints.extend(get_checkpoints_frombackup(filepath))
                except Exception, e:
                    self.logger.log("FAILED: sqlite file %s is corrupt (%s)" %(backup_filepath, str(e)))
                    raise Exception("Sqlite file corrupt")
                self.backup_queue.put((filepath, buffer_path))
                self.logger.log("Added %s to upload queue" %filepath)
            except Exception, e:
                self.logger.log(str(e))
                self.logger.log("FAILED: Creating Backup for %s" %(datetimestamp))
                for b,f in bf_instance.list_splits():
                    self._remove_file(f)
                    self._ss_remove_file(buffer_path, f)
                    self.exit(1)


        checkpoints.sort()
        if os.path.exists(consts.LAST_CHECKPOINT_FILE) and not full_backup:
            f = open(consts.LAST_CHECKPOINT_FILE)
            last_backup_checkpoint = int(f.read())
            f.close()
        else:
            last_backup_checkpoint = 0


        if len(checkpoints):
            if last_backup_checkpoint > 0:

                if last_backup_checkpoint + 1 != checkpoints[0]:
                    self.logger.log("FAILED: Invalid backup. Last backup checkpoint = %d, New backup checkpoint = %s" %(last_backup_checkpoint, checkpoints[0]))
                    self.exit(1)
                else:
                    self.logger.log("Last backup_checkpoint: %d Current backup checkpoints: %s" %(last_backup_checkpoint, str(checkpoints)))

            f = open(consts.LAST_CHECKPOINT_FILE, 'w')
            f.write(str(checkpoints[-1]))
            f.close()

        end_time = time.time()
        time_taken = end_time - start_time
        self.logger.log("Completed Backup for %s" %(datetimestamp))
        self.logger.log("BACKUP SUMMARY: type:%s size:%d, time-taken: %d, backup-file:%s split-count:%d" %(btype, total_size, int(time_taken), backup_filepath, len(bf_instance.list_splits())))
        self.logger.log("==== END BACKUP ====")
        return True

    def mainloop(self):
        self.resume_process()
        backup_thread = Thread(target=self.backup_process_thread)
        upload_thread = Thread(target=self.upload_process_thread)
        backup_thread.setDaemon(True)
        upload_thread.setDaemon(True)
        backup_thread.start()
        upload_thread.start()
        while True:
            time.sleep(100)

    def upload_only_loop(self):
        self.upload_only_mode = True
        self.resume_process()
        self.upload_process_thread()

if __name__ == '__main__':
    if os.getuid() != 0:
        print "Please run as root"
        sys.exit(1)

    try:
        command = sys.argv[1]
    except:
        command = None

    if command == 'start' or command == 'start-with-fullbackup':
        if os.path.exists(consts.MBBACKUP_PID_FILE):
            pid = int(open(consts.MBBACKUP_PID_FILE).read())
            try:
                os.kill(pid, 0)
                print "Uploader Daemon with PID:%d is already running" %pid
                os._exit(1)
            except:
                pass

        try:
            pid = os.fork()
            if pid == 0:
                f = open(consts.MBBACKUP_PID_FILE,'w')
                f.write(str(os.getpid()))
                f.close()
	    else:
                os._exit(0)
        except:
            os._exit(2)

        core = BackupProcess()
        if command == 'start-with-fullbackup':
            core.first_master_backup = True

        core.mainloop()
    elif command == "stop":
        if os.path.exists(consts.MBBACKUP_PID_FILE):
            print "Issuing stop command for Backup Daemon"
            pid = int(open(consts.MBBACKUP_PID_FILE).read())
            try:
                os.kill(pid, 0)
            except:
                print "Backup daemon is not already running"
                sys.exit(0)

            os.kill(pid, signal.SIGTERM)
        else:
           print "Uploader Daemon is not running"
           os._exit(1)

    elif command == 'upload-only':
        core = BackupProcess()
        core.upload_only_loop()
        sys.exit(0)

    else:
        print "Usage: %s command\ncommand = start or stop\n" %sys.argv[0]
